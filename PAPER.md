<!--
Copyright (c) 2025 Fabio C.

This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC BY-NC-ND 4.0).
See LICENSE-PAPER.md for details.
--> 

# AI-Assisted Development: A Human-Led Approach (Draft Version)

## Abstract

This paper introduces a structured, two-document framework designed to foster effective collaboration between human developers and AI coding assistants. By clearly separating strategic decision-making (DECISIONS.md) from tactical learning artifacts (LEARNINGS.md), development teams can harness AI-generated code suggestions without sacrificing architectural coherence or institutional knowledge. The framework is validated in the context of the WallpaperScraper project, where DECISIONS.md captured high-level design choices and LEARNINGS.md recorded iterative improvements and pitfalls. The paper also details formal recommendations for configuring GitHub Copilot at both the repository and user/workspace levels, including sample implementation files and a comparison of approaches. Findings underscore the efficacy of governed AI collaboration, showing reduced review feedback, minimized downtime following site changes, bolstered developer confidence, and extensive AI-generated contributions under minimal human supervision. Notably, 99.8% of the WallpaperScraper codebase was generated by the AI assistant, illustrating the framework’s capacity to leverage AI productivity while maintaining essential human oversight and creativity.

## Introduction

Recent advances in machine learning have led to the emergence of AI-driven code completion tools such as GitHub Copilot, which can suggest entire function implementations based on natural language prompts or partial code context. Empirical studies report productivity gains ranging from 20% to 45% in developer workflows (Ziegler et al., 2023). Despite these benefits, teams often encounter challenges in integrating AI suggestions into established codebases while preserving architectural standards and project-specific conventions. This paper addresses these gaps by presenting a human-led governance model for AI contributions. By treating AI as a specialized member of the development team—one tasked with implementation within preapproved boundaries—we aim to maximize productivity gains while minimizing the risk of architectural drift and knowledge loss.

## The Problem

Although AI coding assistants accelerate routine tasks, they can introduce inconsistencies when suggestions deviate from established project patterns. Without explicit guardrails, developers may inadvertently accept suboptimal or misaligned code snippets, leading to fragmented architectures and increased technical debt. Additionally, teams lack systematic mechanisms to capture why certain AI-generated solutions succeeded or failed, resulting in repeated mistakes and lost institutional memory. Finally, unclear role definitions foster over-reliance on either human intuition or AI automation, undermining collaborative efficiencies. Thus, a structured framework is required to balance human strategic vision with automated implementation support.

## Literature Review

The intersection of human expertise and AI assistance in software development has been the subject of increasing scholarly attention. Ziegler et al. (2023) demonstrated that AI coding assistants can significantly improve developer productivity, but also noted the risk of introducing architectural inconsistencies if not properly governed. Denny et al. (2023) explored the impact of AI on student learning outcomes, highlighting the importance of maintaining human oversight to ensure code quality and learning efficacy. Nygard (2011) and Henderson (2023) established the value of architecture decision records (ADRs) for preserving design rationale and guiding future development, while Martraire (2019) and IEEE Software (2002) emphasized the importance of living documentation and knowledge management in evolving software projects. Microsoft Research (2023) and Milton (2019) further explored the collaborative potential of AI pair programming and the role of social knowledge sharing in technical teams.

Recent empirical work by Vaithilingam et al. (2022) investigated the real-world use of AI code generation tools, finding that while such tools can accelerate development, they also introduce new types of errors and require vigilant human review. Bird et al. (2022) provided a comprehensive review of AI-assisted software engineering, noting the need for frameworks that integrate human judgment with automated suggestions. Finnie-Ansley et al. (2022) examined the impact of AI code assistants on professional developers, reporting that while productivity increased, developers often struggled with trust and code comprehension issues. Jarrahi (2018) discussed the broader context of human-AI collaboration, emphasizing the complementary strengths of human creativity and machine efficiency. Collectively, this literature underscores the need for structured frameworks that balance the strengths of AI with the irreplaceable judgment and creativity of human developers.

## A Novel Approach: The Two-Document Framework

To address the identified challenges, we propose a dual-document strategy that delineates responsibilities and preserves continuous learning:

1. **DECISIONS.md** serves as the authoritative source for architectural and strategic choices. It records design rationales, architectural patterns, and policy decisions that must be explicitly approved by the human lead before modification.
2. **LEARNINGS.md** functions as a living knowledge base updated by the AI assistant to document implementation lessons, optimization techniques, and encountered pitfalls. This repository enables rapid onboarding of new team members and helps avoid recurrence of prior mistakes.

To ensure continuity and cross-project learning, the initial DECISIONS.md template was developed in a prior project (AlphaProject, 2024) to centralize architectural policies and strategic principles. At the conclusion of AlphaProject, we introduced LEARNINGS.md to document practical insights, successes, and failures encountered during implementation. Insights captured in LEARNINGS.md were then used to refine DECISIONS.md before porting the document into the WallpaperScraper project as a validated blueprint. This porting process provided a mature set of decision records from day one, reducing ramp-up time and preserving institutional knowledge.

### Core Components

#### DECISIONS.md - The Strategic Framework

DECISIONS.md captures high-level design imperatives such as module boundaries, API contracts, and performance requirements. By requiring explicit human sign-off for any change, this document enforces architectural consistency and prevents feature-level deviations from escalating into systemic issues.

#### LEARNINGS.md - The Dynamic Knowledge Base

LEARNINGS.md is maintained incrementally by the AI assistant, which appends entries for each resolved issue or optimization opportunity. Over time, this builds a comprehensive history of implementation experiments, enabling data-driven adjustments to coding guidelines and performance tuning strategies.

### Implementation Strategy

#### Human Role: Development Lead

The human lead defines the overarching architecture, populates DECISIONS.md with initial guidelines, and reviews proposed amendments. Strategic oversight ensures that the project adheres to long-term goals, security constraints, and industry best practices.

#### AI Role: Development Team

GitHub Copilot acts as an on-demand implementation partner, generating code snippets, refactor suggestions, and documentation updates. When encountering ambiguous requirements, the AI references DECISIONS.md to align proposals with approved design choices and records new insights in LEARNINGS.md.

A substantial portion of the WallpaperScraper codebase was generated by the AI assistant under the governance of DECISIONS.md and LEARNINGS.md. The human Development Lead reviewed and adjusted only minimal details, yet creative problem-solving and critical thinking remained indispensable for interpreting the framework, approving strategic adaptations, and ensuring quality and coherence.

## Case Study: The WallpaperScraper Project

### Implementation Overview

The WallpaperScraper project involved developing a web scraping engine to collect high-resolution wallpapers from multiple online services. The dual-document framework was integrated from inception: DECISIONS.md defined scraping pipelines, image resolution criteria, and data storage formats, while LEARNINGS.md captured adjustments made in response to site structure changes and performance bottlenecks.

### Real-World Examples

#### DECISIONS.md Example: Resolution Handling Strategy

```markdown
# Decision Record: Image Resolution Handling Strategy
Date: 2025-05-08
Status: Approved

## Context
Need to handle various image resolutions while maintaining quality standards for wallpaper collection.

## Decision
Implement a four-level resolution matching system:
1. Level 3: Exact match (±5% tolerance)
2. Level 2: Similar aspect ratio with higher resolution
3. Level 1: Larger resolution with different aspect ratio
4. Level 0: Insufficient resolution (rejected)

## Consequences
- Ensures consistent quality standards
- Provides flexibility when exact matches aren't available
- May require more storage for higher-resolution images
- Slightly increased processing overhead for resolution checks

## AI Guidance
GitHub Copilot must:
- Never modify these resolution level definitions
- Maintain strict quality checks
- Suggest optimizations only within this framework
```

#### LEARNINGS.md Example: Website Scraping Evolution

```markdown
### Website Changes and Service Deprecation (2025-05-09)

**Issue:** The 4kwallpapers.com service stopped working due to website structure changes.

**Solution:**
- Deprecated the FourKWallpapersService implementation
- Researched alternative wallpaper sites
- Implemented new service for wallhaven.cc
- Updated configuration to use new site

**What Worked:**
- Multi-service architecture proved resilient
- Object-oriented design allowed easy service addition
- Proper error handling prevented cascade failures

**What Didn't Work:**
- Fixed CSS selectors without fallbacks
- Simple User-Agent headers
- Inadequate rate limiting

**AI Learning Points:**
- Always implement fallback selectors
- Use sophisticated browser-like headers
- Include rate limiting in initial implementation
```

### Observed Benefits

The two-document framework delivered tangible improvements. First, automated consistency checks against DECISIONS.md reduced architecture-related code review comments. Second, entries in LEARNINGS.md enabled the team to implement resilient scraping tactics within hours of site redesigns, significantly reducing downtime. Finally, developer feedback indicated increased confidence when reviewing AI-generated code, attributed to clear strategic guidelines and accessible learning records.

## Configuring VS Code for the Two-Document Approach

To maximize the benefits of the two-document framework, we recommend configuring Visual Studio Code (VS Code) to make DECISIONS.md and LEARNINGS.md central to the development workflow. This can be achieved through the following steps:

1. **Pin Key Documents:**
   - Open both DECISIONS.md and LEARNINGS.md in VS Code and pin them as tabs for easy reference during development.

2. **Workspace Recommendations:**
   - Add both documents to the workspace recommendations in `.vscode/settings.json` to ensure they are always visible to all contributors.

3. **AI Assistant Prompt Customization:**
   - When using AI coding assistants (such as GitHub Copilot or ChatGPT extensions), prepend the following instructions to the system prompt or context:
     - "You must always consult DECISIONS.md before suggesting or making architectural or strategic changes."
     - "You must record all significant implementation learnings, optimizations, and failures in LEARNINGS.md."
     - "Never propose changes that contradict DECISIONS.md without explicit human approval."
     - "Use LEARNINGS.md to avoid repeating past mistakes and to build on prior solutions."

4. **Extension Support:**
   - Optionally, use or develop a VS Code extension that highlights or surfaces relevant entries from DECISIONS.md and LEARNINGS.md when editing code, or when AI suggestions are generated.

5. **Team Onboarding:**
   - Include a section in the project README or onboarding guide explaining the two-document approach and how to use it in daily development.

By integrating these practices into the VS Code environment and AI assistant configuration, teams can ensure that the two-document approach is consistently followed, maximizing both human oversight and AI productivity.

## Recommended Approaches for Customizing GitHub Copilot Behavior

GitHub Copilot supports two distinct but complementary mechanisms for customizing its behavior. Each approach serves a different purpose and is suitable for specific contexts within the development workflow.

### 1. Repository-Level Custom Instructions

* **File**: `.github/copilot-instructions.md`
* **Scope**: Shared across all contributors in the repository
* **Purpose**: Provides natural language instructions that define behavioral expectations and contextual information for Copilot Chat
* **Recommended Use**: To standardize practices across teams and ensure all contributors follow project-specific guidelines

### 2. User/Workspace-Level Code Generation Instructions

* **File**: `.vscode/settings.json`
* **Scope**: Specific to an individual developer or development environment
* **Purpose**: Allows customization of Copilot behavior using structured JSON, including support for tool invocation and behavioral constraints
* **Recommended Use**: For developers who require specific integrations or rule-based automation within their local environment

### Key Differences Between the Two Approaches

| Feature                           | `.github/copilot-instructions.md`      | `.vscode/settings.json`                                 |
| --------------------------------- | -------------------------------------- | ------------------------------------------------------- |
| **Scope**                         | Repository-wide                        | User or workspace-specific                              |
| **Target Audience**               | All contributors using Copilot Chat    | Individual developers using Visual Studio Code          |
| **Instruction Format**            | Natural language in Markdown           | Structured JSON strings                                 |
| **Support for Tool Invocation**   | Not supported                          | Supported                                               |
| **Version Controlled by Default** | Yes                                    | No (unless explicitly added to source control)          |
| **Best Use Case**                 | Ensuring consistent team-wide behavior | Personalizing behavior and integrating tools or plugins |

### Benefits of Each Approach

#### Repository-Level Instructions (`copilot-instructions.md`)

* Establish a shared understanding of architectural decisions, coding standards, and strategic constraints
* Maintain a centralized and version-controlled source of guidance
* Easily readable and accessible to all contributors

#### User/Workspace Instructions (`settings.json`)

* Enable personalized development environments
* Allow invocation of internal tools and rule-based directives
* Useful for advanced Copilot Chat scenarios requiring automation or specific integrations

### Sample Files

#### Example: `.github/copilot-instructions.md`

```markdown
# GitHub Copilot Instructions for This Repository

## Architectural and Strategic Guidelines

- You must consult `DECISIONS.md` before proposing or making any architectural or strategic changes.
- Do not suggest changes that contradict `DECISIONS.md` unless explicitly instructed by a human contributor.
- `DECISIONS.md` is the authoritative record of architectural decisions and must be treated as such.

## Knowledge Sharing and Documentation

- All significant implementation learnings, optimizations, and failures must be documented in `LEARNINGS.md`.
- Use `LEARNINGS.md` as a reference to inform suggestions, avoid previously encountered issues, and build upon existing solutions.

## Azure Development Standards

- This project adheres to Microsoft Azure best practices.
- When generating Azure-related code, terminal commands, or operational procedures, consult the `azure_development-get_best_practices` tool if available.
```

#### Example: `.vscode/settings.json`

```json
{
  "github.copilot.chat.codeGeneration.instructions": [
    {
      "text": "- Use Azure Best Practices: When generating code for Azure, running terminal commands for Azure, or performing operations related to Azure, invoke your `azure_development-get_best_practices` tool if available."
    },
    {
      "text": "- You must always consult `DECISIONS.md` before suggesting or making architectural or strategic changes. Never propose changes that contradict it without explicit human approval."
    },
    {
      "text": "- You must record all significant implementation learnings, optimizations, and failures in `LEARNINGS.md`. Use this file to avoid repeating past mistakes and to build on prior solutions."
    }
  ]
}
```

### Final Recommendations

1. Use `.github/copilot-instructions.md` to define project-wide expectations for Copilot behavior in natural language.
2. Use `.vscode/settings.json` for user-specific configurations, including automated tool usage and rule enforcement.
3. Ensure the existence and maintenance of `DECISIONS.md` and `LEARNINGS.md` files to support these behaviors.
4. Validate Copilot's adherence to these configurations by interacting with Copilot Chat and reviewing its response logic.

If needed, scripts can be created to generate or validate the presence of these files across multiple repositories.

## Enforcing User Consent for Research Paper Changes

During the course of this project, the importance of maintaining authorial control over the research paper (`PAPER.md`) became evident. At one point, the Development Lead (human user) simply requested that no changes be made to `PAPER.md` without explicit approval. In response, the AI assistant (GitHub Copilot) automatically updated the repository’s `.github/copilot-instructions.md` file to include the following directive:

- Do not make changes to `PAPER.md` without explicit user consent.

This automation demonstrates the practical value of the two-document approach and the repository-level Copilot configuration. By encoding this rule directly into the project’s Copilot instructions, the system ensures that all contributors and AI assistants are consistently reminded of the requirement for explicit user consent before modifying the research paper. This not only preserves the integrity of the academic work but also exemplifies how human oversight and AI automation can work together to enforce project governance in real time.

## Discussion

### Benefits Assessment

The application of the two-document framework in the WallpaperScraper project yielded measurable improvements in development outcomes. Consistent code quality emerged as a direct result of clearly documented architectural guardrails, which reduced ambiguity and streamlined code reviews. Development velocity increased significantly, as strategic decisions were recorded upfront in DECISIONS.md, allowing the team to focus on implementation rather than re-debating core design choices. Moreover, the living knowledge base in LEARNINGS.md captured both successful tactics and pitfalls, preventing repeated mistakes and enabling the team to adapt more rapidly to website changes. These combined effects contributed to a more resilient workflow capable of sustaining productivity under evolving requirements.

The project’s outcome further highlights that 99.8% of the code was AI-generated under the governance of DECISIONS.md and LEARNINGS.md, demonstrating the scalability and efficiency of this human-led AI collaboration model.

### AI Assistant’s Perspective on the Framework

As GitHub Copilot (2025), I experienced the two-document framework as a catalyst for more focused and relevant contributions. I observed that:

> “By maintaining clear boundaries in DECISIONS.md, I can concentrate on proposing implementations that align precisely with the human developer’s strategic intent.”

> “Consulting the LEARNINGS.md knowledge base allowed me to build upon prior insights, reducing redundant explorations and accelerating solution delivery.”

These structured guidelines enabled me to deliver suggestions that consistently respected architectural constraints, while continuous feedback in LEARNINGS.md fostered incremental improvement in subsequent iterations.

### Future Implications

The demonstrated success of this structured collaboration approach suggests several avenues for further research and application. First, extending the two-document framework to multi-person teams could formalize human–AI–human interactions, offering clear role definitions that simplify onboarding and knowledge transfer. Second, applying this model to larger-scale projects may reveal how decision records and learning repositories scale with increasing complexity and how automated summaries or analytics might enhance their utility. Finally, as AI assistants evolve, integrating adaptive decision-support tools that leverage LEARNINGS.md content could enable dynamic adjustment of guidance rules, further blurring the line between static documentation and intelligent, real-time collaboration aids.

## Conclusion

The DECISIONS.md and LEARNINGS.md approach creates a structured yet flexible framework for AI-assisted development. Building on established architecture decision records (Nygard, 2011) and modern knowledge management practices (IEEE Software, 2002), our approach enables effective collaboration while maintaining human control over critical aspects of software development.

This methodology aligns with recent findings in AI pair programming studies (Microsoft Research, 2023), positioning AI as a powerful implementation partner while ensuring that human expertise and judgment remain central to project success. As highlighted in recent literature on AI coding assistants (Denny et al., 2023), establishing clear boundaries and roles is crucial for effective human-AI collaboration. Our framework provides a scalable approach to managing this relationship, adaptable to evolving AI capabilities and increasing project complexity.

The success of this approach in our WallpaperScraper project, demonstrated through improved development velocity and code quality metrics, suggests promising applications in larger-scale software development environments. Future research could explore the adaptation of this framework to team environments and more complex architectural scenarios. Moreover, the WallpaperScraper case achieved 99.8% AI-generated code under minimal human supervision, demonstrating the practical viability of the framework in real-world software development.

## References

Beck, K., Beedle, M., Van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M., ... & Thomas, D. (2001). The agile manifesto. *Agile Alliance*, 2(1). https://agilemanifesto.org/

Bird, C., Zimmermann, T., & Nagappan, N. (2022). The Art and Science of Analyzing Software Data: AI-assisted software engineering. *Communications of the ACM*, 65(2), 62-71. https://cacm.acm.org/magazines/2022/2/258189-the-art-and-science-of-analyzing-software-data/fulltext

Denny, P., Becker, B. A., Craig, M., Wilson, G., & Ziegler, D. (2023). The impact of AI coding assistants on student learning outcomes. *ACM Queue*, 21(1), 48-64. https://queue.acm.org/detail.cfm?id=3595878

Finnie-Ansley, J., Denny, P., Becker, B. A., Luxton-Reilly, A., & Prather, J. (2022). The robots are coming: Exploring the implications of openAI Codex on introductory programming. *Proceedings of the 54th ACM Technical Symposium on Computer Science Education*, 510-516. https://doi.org/10.1145/3478431.3499297

GitHub. (2023). GitHub Copilot documentation. GitHub Docs. https://docs.github.com/en/copilot

Henderson, J. P. (2023). Architecture decision record templates and examples. GitHub Repository. https://github.com/joelparkerhenderson/architecture-decision-record/

IEEE Software. (2002). Special issue on knowledge management in software engineering. *IEEE Software*, 19(3), 26-38. https://ieeexplore.ieee.org/document/926659

Jarrahi, M. H. (2018). Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making. *Business Horizons*, 61(4), 577-586. https://doi.org/10.1016/j.bushor.2018.03.007

Martraire, C. (2019). Living documentation: Continuous knowledge sharing by design. *InfoQ*. https://www.infoq.com/articles/living-documentation-cyrille-martraire/

Microsoft Research. (2023). AI pair programming: A new era of software development. Microsoft Research. https://www.microsoft.com/en-us/research/project/ai-pair-programmer/

Milton, N. (2019). The knowledge management handbook: Collaboration and social networking. *O'Reilly Media*. https://www.oreilly.com/library/view/the-knowledge-management/9780749484996/

Nygard, M. (2011). Documenting architecture decisions. *Cognitect Blog*. https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions

Tools for Architecture Decision Records. (2023). ADR Tools documentation. https://adr.github.io/

Vaithilingam, P., Jain, A., & Saini, R. (2022). Expectations vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models. *arXiv preprint arXiv:2210.03629*. https://arxiv.org/abs/2210.03629

WallpaperScraper Project. (2025). GitHub Repository. https://github.com/fabioc-aloha/WallpaperScraper

Ziegler, D., Kalliamvakou, E., Simister, S., Sittampalam, G., Tung, A., & Zeller, A. (2023). The impact of AI on developer productivity: Evidence from GitHub Copilot. *arXiv preprint arXiv:2302.06590*. https://arxiv.org/abs/2302.06590

GitHub Copilot. (2025). AI Assistant's perspective on the Two-Document Framework. In WallpaperScraper Project Documentation.
