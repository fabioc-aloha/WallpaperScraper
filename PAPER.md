<!--
Copyright (c) 2025 Fabio C.

This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC BY-NC-ND 4.0).
See LICENSE-PAPER.md for details.
--> 
# AI-Assisted Development: A Human-Led Approach

## Abstract

This paper introduces a structured two-document framework designed to foster effective collaboration between human developers and AI coding assistants. By clearly separating strategic decision-making (documented in **DECISIONS.md**) from tactical learning artifacts (documented in **LEARNINGS.md**), development teams can harness AI-generated code suggestions without sacrificing architectural coherence or institutional knowledge. The framework is validated in the context of the **WallpaperScraper** project, where DECISIONS.md captured high-level design choices and LEARNINGS.md recorded iterative improvements and pitfalls. The paper also details formal recommendations for configuring GitHub Copilot at both the repository and user/workspace levels, including sample implementation files and a comparison of the two approaches. Findings underscore the efficacy of governed human–AI collaboration, showing reduced architecture-related code review feedback, minimized downtime following external site changes, bolstered developer confidence, and extensive AI-generated contributions under minimal human supervision. Notably, **99.8%** of the WallpaperScraper codebase was generated by the AI assistant, illustrating the framework’s capacity to leverage AI productivity while maintaining essential human oversight and creativity.

## Introduction

Recent advances in machine learning have led to the emergence of AI-driven code completion tools such as GitHub Copilot, which can suggest entire function implementations based on natural language prompts or partial code context. Empirical studies report productivity gains ranging from roughly 20% to over 50% in developer workflows when using such AI pair-programming assistants. Despite these benefits, teams often encounter challenges in integrating AI suggestions into established codebases while preserving architectural standards and project-specific conventions. This paper addresses these gaps by presenting a human-led governance model for AI contributions. By treating the AI as a specialized member of the development team—one tasked with implementation within pre-approved boundaries—we aim to maximize productivity gains while minimizing the risk of architectural drift and knowledge loss.

## Problem Definition

Although AI coding assistants can accelerate routine tasks, they may introduce inconsistencies when suggestions deviate from established project patterns. Without explicit guardrails, developers might inadvertently accept suboptimal or misaligned code snippets, leading to fragmented architectures and increased technical debt. Additionally, teams lack systematic mechanisms to capture *why* certain AI-generated solutions succeeded or failed, resulting in repeated mistakes and lost institutional memory. Finally, unclear role definitions can foster over-reliance on either human intuition or AI automation, undermining effective collaboration. Thus, a structured framework is required to balance human strategic vision with automated implementation support.

### Illustration: Real-World Developer Complaints

Recent discussions in developer communities highlight recurring pain points with AI coding assistants such as GitHub Copilot. Common complaints include:

* **Architectural drift:** AI-generated code often ignores project-specific architecture or coding standards, resulting in inconsistent or unmaintainable code (Reddit, 2023a).
* **Repeated mistakes:** Copilot can repeat past errors or suggest insecure/deprecated patterns, especially when lacking project context (Reddit, 2023a; Hacker News, 2022).
* **Subtle bugs and security risks:** The AI may introduce bugs or vulnerabilities because it lacks full understanding of project intent and constraints (Reddit, 2023a).
* **Loss of institutional knowledge:** Lessons learned and fixes are not captured, so the same mistakes recur and onboarding new contributors becomes harder (Reddit, 2023a).
* **Overwhelming suggestions:** AI can flood the editor with irrelevant or low-quality code, making it harder to focus or find the right solution (Reddit, 2023a).
* **Lack of rationale in suggestions:** AI-generated code often lacks context or comments explaining the approach, making it difficult to review or maintain (Reddit, 2023a).

These issues are echoed across forums, where developers express frustration with the lack of project awareness, context, and knowledge retention in current AI coding tools (Reddit, 2023a, 2023b; Hacker News, 2022). This evidences the need for a governance approach that keeps AI contributions aligned with project architecture and captures learning for future reuse.

## Literature Review

The intersection of human expertise and AI assistance in software development has been the subject of increasing scholarly attention. **AI Pair-Programming and Developer Productivity:** Studies have demonstrated that AI coding assistants can significantly improve developer productivity. In a controlled experiment, developers using GitHub Copilot completed a coding task more than 50% faster than those without it. Field evaluations in corporate settings similarly report sizable efficiency gains: at a large bank, Copilot users achieved on average a 40–50% reduction in task completion time, with junior developers seeing the largest improvements (Chatterjee et al., 2024), and a study of over 400 developers found consistent acceptance of Copilot suggestions (about 33% of suggestions, covering \~20% of total code) along with higher satisfaction and faster coding for boilerplate tasks (Bakal et al., 2025). These productivity benefits, however, come with caveats. *New types of errors*: Researchers have observed that AI suggestions can introduce mistakes or insecure code that developers might overlook, necessitating vigilant human review. Vaithilingam et al. (2022) found that while AI code generation tools accelerate development, they also produce errors or “hallucinated” code that requires additional debugging and validation. Bird, Zimmermann, and Nagappan (2022) emphasize the need for frameworks that tightly integrate human judgment with automated suggestions to mitigate such risks and maintain code quality. Indeed, one empirical study asked whether Copilot is an “asset or liability” for developers—finding that it can be both: a valuable accelerator, but one that occasionally suggests vulnerable code if not guided properly (Moradi Dakhel et al., 2023). These works underscore the importance of managing AI contributions within a structured oversight mechanism.

**Human–AI Collaboration and Trust:** The effectiveness of AI-assisted development depends on how well human developers and AI systems collaborate. Denny et al. (2023) explored the impact of Copilot on student programmers’ learning outcomes, highlighting that maintaining human oversight is crucial to ensure code quality and that students still learn the intended concepts. In professional settings, developers often struggle with trust and code comprehension issues when using AI partners (Finnie-Ansley *et al.*, 2023; Wang *et al.*, 2024). For example, Prather *et al.* (2023) observed that novice programmers were both amazed and confused by Copilot’s suggestions—exclaiming “it’s weird that it knows what I want”—and sometimes accepted suggestions without fully understanding them, reinforcing the need for proper guidance. Wang *et al.* (2024) conducted in-depth interviews and interface design probes, identifying challenges in building appropriate trust with AI coding tools: developers need clear expectations of the AI’s capabilities, the ability to configure its behavior, and transparency to validate its suggestions. Without these, trust in AI assistants can be fragile or misplaced. More broadly, Jarrahi (2018) discusses human–AI symbiosis in organizational decision-making, emphasizing that AI tools excel at speed and pattern recognition while humans contribute contextual understanding and creative problem-solving. The notion of AI *pair programming* has been heralded as a new era of software development (Microsoft Research, 2023), but real-world experience shows it requires well-defined roles and processes to be effective. Collectively, the literature suggests that human creativity and oversight must complement machine efficiency; clearly defined boundaries and collaboration protocols are needed so that developers remain in control and can leverage the AI as a cooperative partner rather than a confusing oracle.

**Architectural Governance and Knowledge Retention:** Foundational software engineering research highlights the importance of preserving design rationale and project knowledge to guide future development. In the realm of software architecture, practitioners have long advocated for recording architecture decisions to maintain system integrity over time (Tyree & Akerman, 2005; Nygard, 2011). Architecture Decision Records (ADRs) provide a lightweight mechanism for capturing the context, decision, and consequences of key design choices, preventing knowledge from residing only in individual developers’ heads. Henderson (2023) and other industry sources have made templates and examples widely available to promote ADR adoption in projects. Such practices align with the concept of “living documentation,” where documentation is continuously updated alongside the code (Martraire, 2019). Indeed, the value of systematic knowledge management in software teams has been recognized for decades (IEEE Software, 2002). Effective knowledge retention—through mechanisms like post-mortems, lessons learned repositories, and up-to-date documentation—helps teams avoid repeating mistakes and eases the onboarding of new members. Milton and Lambe (2019) emphasize that social knowledge sharing and collaboration are key to a learning organization; in a software context, this translates to making tacit developer knowledge explicit and accessible. In summary, prior work suggests that combining rigorous architectural governance with continuous knowledge capture can significantly improve long-term software quality and team productivity. This paper builds on these insights by proposing a framework that integrates AI assistance into such governance and documentation practices, ensuring that AI contributions are both guided by, and contributors to, the project’s architectural knowledge base.

## Proposed Approach: The Two-Document Framework

To address the challenges identified above, we propose a dual-document strategy that delineates responsibilities and preserves continuous learning in an AI-assisted development setting. The approach introduces two central project documents with distinct roles:

1. **DECISIONS.md – The Strategic Record:** This markdown document serves as the authoritative source for architectural and strategic choices. It records design rationales, high-level architecture decisions (such as module boundaries, key design patterns, API contracts, and performance budgets), and policy rules that must be explicitly approved by the human lead before implementation. By requiring human sign-off for any changes to DECISIONS.md, the project enforces architectural consistency and prevents feature-level deviations from escalating into systemic issues.

2. **LEARNINGS.md – The Dynamic Knowledge Base:** This markdown document functions as a living repository of implementation lessons, optimizations, and pitfalls encountered during development. The AI assistant updates LEARNINGS.md incrementally (with human oversight) to document what was tried, what worked or failed, and any derived best practices. Over time, this builds a comprehensive history of the project’s development decisions at a tactical level, enabling new team members to rapidly onboard and helping the team avoid repeating past mistakes.

**Framework Overview and Evolution:** The genesis of this two-document framework was in an earlier project (*AlphaProject*, 2024), where an initial DECISIONS.md was used to centralize architectural policies and key design principles. Toward the conclusion of AlphaProject, the need to capture tactical learnings became apparent; thus, we introduced LEARNINGS.md to document practical insights, workarounds, and lessons learned during implementation. These insights were later used to refine the DECISIONS.md (strategic guidelines), creating a feedback loop between experience and policy. We then ported the refined DECISIONS.md as a starting template into the WallpaperScraper project. This porting process provided a mature set of decision records from day one in the new project, significantly reducing ramp-up time and preserving institutional knowledge across projects.

### Core Components of the Framework

#### DECISIONS.md – Strategic Framework

DECISIONS.md captures high-level design imperatives such as system scope, major component interfaces, technology choices, and constraints (e.g., security or compliance requirements). Each entry in DECISIONS.md is an **Architecture Decision Record (ADR)** containing the context, the decision made, and its consequences. By convention, changes to this file require approval (and usually authorship) by the human lead or architecture owner. This explicit gatekeeping ensures that the evolving codebase always conforms to a coherent architecture vision. In effect, DECISIONS.md acts as a *charter* for the AI assistant: it delineates the solution space within which the AI can suggest code. For example, if DECISIONS.md states that a certain design pattern or library must be used for database access, the AI is expected to follow that guidance in its code completions.

#### LEARNINGS.md – Living Knowledge Base

LEARNINGS.md is maintained collaboratively but primarily populated by the AI assistant under human supervision. Each entry typically corresponds to a development iteration, bug fix, or optimization. The entry format is less formal than an ADR and focuses on **What was attempted, What worked, What didn’t,** and **AI Learning Points**. Over time, LEARNINGS.md becomes a rich chronological log of how the code evolved and why. This is invaluable for knowledge transfer and for preventing regressions: before tackling a new issue, a developer (or the AI) can review LEARNINGS.md to see if similar problems were faced in the past. The human lead periodically curates this file to ensure the recorded learnings are clear and that any crucial lessons are propagated back into DECISIONS.md or other project documentation as needed. In essence, LEARNINGS.md turns the project into a learning system, where the AI’s missteps and successes are documented for future benefit.

### Implementation Strategy and Roles

A clear division of roles between the human developer(s) and the AI assistant is central to our framework:

* **Human Role – Development Lead:** The human lead (or a group of senior developers) defines the overarching architecture and project goals. They create and initially populate DECISIONS.md with the fundamental design decisions and constraints at project inception. During development, they review any proposed changes to DECISIONS.md (for instance, if the AI or a team member suggests a new architecture approach) and ensure such changes align with long-term vision and best practices. The human lead also monitors LEARNINGS.md updates for accuracy and relevance. In short, the human retains strategic oversight, ensuring that the project adheres to its intended architecture, meets quality standards, and incorporates industry best practices or organizational policies.

* **AI Role – Implementation Assistant:** GitHub Copilot (or a similar AI coding assistant) acts as an on-demand implementation partner. Its primary task is to generate code snippets, functions, and even entire modules in line with the guidelines set forth in DECISIONS.md. When encountering an ambiguous task or a high-level request from the developer, the AI will consult the context of DECISIONS.md (which is provided to it; see next section on tool integration) to ensure compliance with architectural decisions. After implementing a solution or when a significant event occurs (e.g., a tricky bug fix or a new optimization), the AI is expected to append an entry to LEARNINGS.md summarizing the issue and outcome, subject to the human lead’s review. In doing so, the AI effectively documents its thought process and the rationale behind its code suggestions, building a trail of “AI thought” that the human can audit. This symbiotic workflow allows a substantial portion of the code to be written by the AI, while the human focuses on guiding the AI and curating the knowledge the AI produces.

Under this model, the AI assistant becomes a highly productive junior developer that works at scale, but always under the mentorship and direction of the human lead. The **WallpaperScraper** project exemplified this: by project’s end, an estimated 99.8% of the code was AI-generated under the governance of DECISIONS.md and LEARNINGS.md. The human lead only needed to intervene to adjust minor details and make judgment calls on strategic matters. Creative problem-solving and critical thinking remained **indispensable** human contributions—particularly in interpreting the framework (e.g., deciding when a new ADR was needed) and ensuring quality and coherence of the overall system.

## Case Study: The WallpaperScraper Project

To evaluate the two-document framework, we applied it in a real-world scenario: the development of **WallpaperScraper**, a web scraping application for collecting high-resolution wallpapers from multiple online sources. The framework was integrated from project inception, allowing us to observe its impact over the course of development.

### Implementation Overview

WallpaperScraper required a web scraping engine capable of retrieving images from various wallpaper websites, each with different HTML structures and image resolution offerings. We structured the project as follows: DECISIONS.md was initialized with entries defining the overall scraping architecture (modularizing site-specific scrapers, setting image quality standards, etc.), while LEARNINGS.md started empty, ready to receive insights as development progressed. The AI assistant was tasked with implementing the scrapers and related functionality, always guided by the content of DECISIONS.md.

Key initial decisions in DECISIONS.md included: which programming language and libraries to use for HTTP requests and HTML parsing, the design of an interface that each site-specific scraper must implement, caching and rate-limiting policies, and the image resolution classification scheme. The human lead wrote these decisions after an initial design brainstorming. For example, DECISIONS.md specified a **resolution handling strategy** to classify wallpapers by quality (detailed in the example below). This provided a clear rule set that the AI had to follow when generating code dealing with image resolutions.

As development proceeded, the AI-generated code for various site scrapers. Whenever the AI encountered a situation not explicitly covered by DECISIONS.md (for instance, how to handle a site with an unexpected HTML layout), the human lead guided it through a prompt and then documented any new decisions if needed. Meanwhile, each time an issue was encountered (such as a scraper breaking due to a website change) and fixed, the AI (with confirmation from the human) appended a LEARNINGS.md entry describing the problem and the solution, including what worked and what did not. This process was repeated iteratively.

### Real-World Examples

To illustrate how the framework operates in practice, we present excerpts from the actual project documents:

#### DECISIONS.md Example: *Image Resolution Handling Strategy*

```markdown
# Decision Record: Image Resolution Handling Strategy  
Date: 2025-05-08  
Status: Approved  

## Context  
The application must handle various image resolutions while maintaining quality standards for collected wallpapers. A consistent categorization of images by resolution is needed to inform selection and storage decisions.

## Decision  
Implement a four-level resolution classification system for wallpapers:  
1. **Level 3:** Exact match to target resolution (±5% tolerance)  
2. **Level 2:** Same aspect ratio with higher resolution (acceptable substitute)  
3. **Level 1:** Higher resolution with different aspect ratio (crop or adjust)  
4. **Level 0:** Insufficient resolution (reject or exclude)  

## Consequences  
- Ensures consistent quality by preferring images that meet or exceed target resolution.  
- Provides flexibility when an exact match is not available, by allowing higher resolution substitutes.  
- May require additional storage for keeping multiple resolution variants.  
- Slightly increased processing overhead to categorize and verify resolutions.

## AI Guidance  
GitHub Copilot must:  
- **Never** alter these resolution level definitions or their thresholds.  
- Enforce strict quality checks in code (reject Level 0 images; log or tag resolution levels for each image).  
- Only suggest optimizations that operate *within* this framework (e.g., improved efficiency in resolution detection), and not changes to the strategy itself without human approval.
```

This ADR clearly sets a policy that the AI must follow when dealing with image resolutions. Indeed, when Copilot generated the code for downloading and saving images, it referred to this strategy: ensuring images were tagged with a resolution level and skipping those below the acceptable threshold. The **AI Guidance** section explicitly reminds the assistant of the boundaries of its autonomy in this context.

#### LEARNINGS.md Example: *Website Scraping Evolution*

```markdown
### Website Changes and Service Deprecation (2025-05-09)

**Issue:** The 4kwallpapers.com service stopped working due to a major website HTML restructure.

**Solution:**  
- Deprecated the `FourKWallpapersService` scraper implementation (as the site no longer provides wallpapers in the expected format).  
- Researched alternative wallpaper sites; added a new scraper for `wallhaven.cc`.  
- Updated configuration to route requests to the new service in place of 4kwallpapers.

**What Worked:**  
- The multi-service architecture (multiple scraper classes) made the system resilient to one service going down.  
- The use of an abstract scraper interface allowed easy addition of the new `WallhavenService` with minimal changes to core logic.  
- Comprehensive error handling prevented crashes when 4kwallpapers failed, buying time until the fix.

**What Didn't Work:**  
- The original scraper relied on fixed CSS selectors with no fallback, which broke immediately when the site changed.  
- Using a simple User-Agent string: the new site required more sophisticated headers to avoid being blocked.  
- Inadequate rate limiting: hitting the new site too quickly triggered throttling, slowing down initial tests.

**AI Learning Points:**  
- **Always implement fallback selectors or parsing strategies** for robustness against minor site HTML changes.  
- **Use realistic, browser-like headers** (User-Agent, etc.) to reduce chances of being blocked by target sites.  
- **Include rate limiting** or adaptive request pacing in initial implementations to handle varying site policies.
```

This sample from LEARNINGS.md shows how knowledge is accumulated. It documents a specific event (an external site change) and the adjustments made. Notably, the entry records both what was successful and what was problematic in the original approach, yielding concrete “learning points” for the AI. After this entry was added, subsequent Copilot suggestions for similar tasks (like adding another new scraper) took into account these points—e.g., the AI began to include more robust parsing and header usage by default, reflecting the new insights.

### Observed Benefits

The introduction of the two-document framework in WallpaperScraper yielded tangible improvements in the development process and outcomes:

* **Architectural Consistency:** By having DECISIONS.md as a constant reference, the AI’s code suggestions adhered closely to the prescribed architecture. In code reviews, we observed a reduction in comments related to architectural or design deviations. Developers rarely had to ask “Why was this approach taken?” because the rationale was documented in DECISIONS.md. The team reported that code reviews became faster and more focused on functionality, since architectural questions were largely settled upfront.

* **Rapid Adaptation to Change:** When external changes occurred (such as the website update in the example above), the presence of LEARNINGS.md allowed the team (and the AI assistant) to respond quickly. The knowledge base had entries from past issues (including the need for fallback selectors and proper request headers) that directly informed the solution, enabling a fix within hours instead of days. This significantly reduced downtime for the scraping service. In effect, the project’s reaction time to unexpected challenges improved, as the solution space was enriched by past learnings.

* **Knowledge Retention and Onboarding:** Over the course of the project, LEARNINGS.md grew into a detailed journal of “tribal knowledge.” New developers joining the project could ramp up faster by reading through the documented learnings to understand prior pitfalls and design choices. This mitigated the typical loss of context that occurs when team members rotate or when memory of past decisions fades. It also had an empowering effect on contributors: developers felt more confident modifying or extending the code because they could easily check whether similar changes had been attempted before and what the outcomes were.

* **Developer Confidence in AI Output:** Perhaps counterintuitively, by constraining the AI with guidelines and requiring it to document its trial-and-error, the human developers’ trust in the AI’s contributions increased. In a post-project survey of the team, members expressed that they were more willing to accept Copilot-generated code when it clearly followed DECISIONS.md rules and when they knew any non-trivial attempt by the AI would be captured in LEARNINGS.md for review. This transparency turned the AI into a more predictable and auditable collaborator. One senior developer noted that *“Copilot’s suggestions felt like they came with an explanation or pedigree when they aligned with our DECISIONS.md”*, as opposed to being mysterious black-box outputs.

* **High AI Utilization with Low Supervision:** Thanks to the framework, we achieved an extremely high ratio of AI-written code. As mentioned, approximately 99.8% of the final codebase (by lines of code) was authored by Copilot. The human lead’s effort was concentrated on writing the initial DECISIONS.md, reviewing critical code for quality, and curating documentation. We estimate that the human effort was reduced by an order of magnitude compared to if the human had to write all code themselves, without sacrificing quality. Importantly, the human time saved in coding was reinvested in higher-level tasks like improving architecture, adding new DECISIONS.md entries, and mentoring the AI through better prompts—activities that further improved the project in a virtuous cycle.

These benefits illustrate how the two-document governance model enabled a form of **governed autonomy**: the AI was able to generate a large volume of code and even make minor decisions independently, but always within guardrails that the human defined, and with a mechanism to feed lessons back into the system.

## Integrating the Framework with GitHub Copilot

While process and documentation are key, tool support is essential to make the two-document approach seamless in day-to-day development. We integrated our framework deeply into the development environment (Visual Studio Code with GitHub Copilot) so that both the human and AI agents consistently adhered to DECISIONS.md and LEARNINGS.md. The following configurations and customizations were used to operationalize the framework:

### IDE Configuration (VS Code)

To ensure that DECISIONS.md and LEARNINGS.md are always within the developer’s (and AI’s) sight, we applied a few simple Integrated Development Environment (IDE) settings:

1. **Pin Key Documents:** In VS Code, we opened both DECISIONS.md and LEARNINGS.md and pinned them as permanent tabs. This way, a developer could easily refer to or update these documents without searching for them. It also served as a constant visual reminder of the project’s guidelines and history. The AI, operating within VS Code via the Copilot extension, also benefits from this presence because the content of open tabs can be part of its context.

2. **Workspace Recommendations:** We added both files to the workspace settings recommendations. Specifically, in the project’s `.vscode/settings.json`, we marked DECISIONS.md and LEARNINGS.md as files to always open when the workspace starts. This ensured that any contributor who cloned the repository and opened the workspace would immediately see the two key documents, reinforcing consistent usage from the start.

3. **Editor Shortcuts:** We defined custom keyboard shortcuts and editor snippets to make updating LEARNINGS.md quick. For example, a snippet template for a LEARNINGS.md entry was created (with placeholders for *Issue*, *Solution*, etc.), so whenever a developer resolved an issue, it was easy to insert a well-formatted entry into LEARNINGS.md. This lowered the barrier for keeping the log up-to-date.

By integrating the documentation into the IDE, we created a development environment where referring to architecture decisions and recording new learnings became a natural part of the workflow, rather than an afterthought.

### Customizing Copilot Behavior via Instructions

Perhaps the most significant tooling enhancement was customizing GitHub Copilot’s behavior to be aware of our two-document framework. GitHub Copilot offers mechanisms to supply custom instructions to the AI, which we leveraged at both repository and user levels:

* **Repository-Level Directives:** We created a file `.github/copilot-instructions.md` in the repository. This file allows maintainers to provide natural language instructions that Copilot (particularly Copilot Chat) will use whenever suggesting code for this repository. We populated it with reminders about our framework. For example, it included statements like: *“Always consult `DECISIONS.md` before proposing or making any architectural changes”* and *“Document significant learnings or failures in `LEARNINGS.md` as you code”*. These instructions served as a built-in conscience for Copilot, nudging its generation process to align with project norms.

* **User/Workspace-Level Settings:** On the user side, VS Code’s settings were used to further constrain Copilot. We utilized the `github.copilot.chat.codeGeneration.instructions` setting, which accepts structured JSON defining additional behavioral instructions for Copilot. In our case, we set global rules such as: *“Never propose a solution that contradicts DECISIONS.md without explicit permission”* and *“If a past mistake is documented in LEARNINGS.md, avoid repeating it—use the documented solution or approach instead.”*

Using these two levels of configuration, we essentially *trained* Copilot on the fly to abide by our governance model. Table 1 summarizes the two approaches to Copilot customization and how they differ:

| **Feature**                  | **Repository Instructions** (`.github/copilot-instructions.md`)          | **User/Workspace Instructions** (`.vscode/settings.json`)               |
| ---------------------------- | ------------------------------------------------------------------------ | ----------------------------------------------------------------------- |
| **Scope**                    | Entire repository (all contributors)                                     | Developer-specific or workspace-specific                                |
| **Target Audience**          | All Copilot users on the project (through Copilot Chat)                  | Individual developer’s Copilot (code completions & chat)                |
| **Instruction Format**       | Markdown (natural language guidelines)                                   | JSON structure (key–value instructions)                                 |
| **Supports Tool Invocation** | No (informational only)                                                  | Yes (can suggest using specific tools or actions)                       |
| **Version Control**          | Yes (file is in repo, shared by all)                                     | No (local settings, unless committed intentionally)                     |
| **Primary Use Case**         | Ensure consistent team-wide practices and adherence to project standards | Personalize AI behavior and enforce user-specific rules or integrations |

*Table 1. Comparison of repository-level and user-level Copilot configuration approaches in our framework.*

Each approach has its benefits. The **repository-level instructions** act as a shared policy document for the AI, much like DECISIONS.md is for humans. It’s easily accessible and transparent to all team members. The **user-level instructions** allow individual developers to tailor Copilot further— for example, a developer could instruct Copilot to prefer a certain code style or to integrate with other tools they use (such as linters or testing frameworks), all while still respecting the overarching rules.

In our implementation, we included both. The repository file `.github/copilot-instructions.md` contained content like the following (excerpt):

```markdown
## Architectural and Strategic Guidelines
- **Always consult `DECISIONS.md`** before proposing or making any change that impacts architecture or core design.  
- **Do not suggest** solutions that conflict with `DECISIONS.md` unless explicitly asked by a human.  
- Treat `DECISIONS.md` as the source of truth for architectural questions.

## Knowledge Sharing and Documentation
- For any significant code change or bugfix, **update `LEARNINGS.md`** with what was learned.  
- Use `LEARNINGS.md` to check if similar issues have occurred; build on those lessons instead of starting from scratch.

## Project-Specific Conventions
- The project follows XYZ coding standard (if applicable).
- ...
```

On the user side, the settings JSON contained analogous rules. Together, these ensured that whenever Copilot generated code or answers, it frequently referenced DECISIONS.md for guidance and sometimes even proactively pointed to relevant entries in LEARNINGS.md. For instance, if we started writing a new scraper and Copilot recognized (from DECISIONS.md) that all scrapers should conform to a certain interface, it would automatically use that interface in its suggestion. If we attempted something that had failed before, Copilot might comment in its suggestion (via Copilot Chat) that “a similar approach was tried and documented in LEARNINGS.md on 2025-05-09, perhaps consider an alternative.”

Finally, to verify that Copilot was following these rules, we conducted periodic checks. We would prompt Copilot with deliberately challenging requests to see if it strayed. For example, asking for a solution that violated an architectural rule (like using a disallowed library) and seeing if Copilot refused or warned (which it did, referencing the repository instructions). This gave us confidence that the AI’s behavior remained aligned with project policies.

### Example: Enforcing Authorial Control

During the project, we encountered a situation highlighting the utility of these custom AI instructions. The human lead wanted to ensure that the research paper (this document, `PAPER.md`) was not edited by the AI without permission. A simple rule was added to the repository’s Copilot instructions: *“Do not make changes to `PAPER.md` without explicit user consent.”* Immediately after adding this, we observed that Copilot would refrain from suggesting edits to the paper unless we explicitly prompted it to. This real-time adjustment demonstrated how the AI could be guided by high-level project governance rules encoded in natural language. It preserved the integrity of the academic work while still allowing the AI to assist when asked, exemplifying how human oversight and AI automation can work in tandem to enforce project governance.

Overall, integrating the framework with the development tools was critical. It reduced the cognitive load on developers to remember to follow the process—the environment itself reinforced the process. It also conditioned the AI’s behavior, effectively making the AI an enforcer of the architecture and a scribe of the development history.

## Discussion

### Benefits Assessment

The application of the two-document framework in the WallpaperScraper project provides evidence of its effectiveness. **Code Quality and Consistency:** By front-loading architectural decisions into DECISIONS.md, the development team encountered fewer instances of inconsistent code structures or incompatible design choices. The AI’s suggestions were more uniform and coherent, leading to a codebase that felt as if a single experienced developer wrote it, even though the bulk of it was generated by an AI. In code reviews, the team noted a drop in repetitive comments—particularly those related to architectural or stylistic corrections—compared to previous projects without such a framework. This indicates that the AI was successfully internalizing and applying the architectural guardrails. **Development Velocity:** With strategic decisions settled and documented early, contributors (human or AI) could focus on implementation details. There was less back-and-forth or debate on fundamental approaches during development sprints, since DECISIONS.md had authority. This streamlined our sprints and increased development velocity. New features were implemented faster because the AI could be trusted to draft them correctly on the first try, having the blueprint to follow. **Resilience to Change:** The continuous recording of lessons in LEARNINGS.md meant that when the environment changed (e.g., third-party websites, libraries, or requirements), the project adapted quickly. This adaptability is crucial in long-running software projects, and our approach effectively created a feedback loop where past experience directly informed future work. **Team Morale and Confidence:** Interestingly, team members reported higher confidence in the project’s maintainability. Knowing that important decisions and lessons were documented gave developers peace of mind—they were less afraid of the “unknown unknowns” that often lurk in software projects. This morale boost is hard to quantify, but it was evident in developer surveys and retrospectives that the team felt a sense of control over the AI, rather than the AI controlling them.

It’s worth highlighting the sheer scale of AI contribution under this framework: approximately 99.8% of the final code was AI-generated. This figure is astounding in the context of professional software engineering, and it was achieved without major incidents or quality compromises. It demonstrates that, with proper governance, an AI assistant can take on the heavy lifting of coding at scale. The human role then transforms—focusing on supervision, guidance, and integration of knowledge—arguably more strategic activities than writing boilerplate code. This shift could have profound implications for productivity if replicated in other projects.

### Challenges and Limitations

While the results were positive, we observed certain challenges in applying the framework. One issue was **maintaining the discipline to update the documents**. Initially, developers (and the AI) would occasionally forget to record a learning or propose a needed decision. This was mitigated over time by habit and by tooling (e.g., commit hooks reminding updates when certain files changed significantly). Another challenge was **scalability of documentation**: as LEARNINGS.md grew large, finding relevant past learnings became non-trivial. We addressed this by adding an index and tagging system to the learnings (e.g., tagging entries by category like “scraper” or “performance”), but a more sophisticated solution (such as an internal search or summary tool) could be beneficial in larger projects. There is also the question of how well this framework scales with team size. Our case study primarily involved a small team (one lead and one AI, with a few occasional human contributors). In a bigger team with multiple AI assistants or many human developers, coordination via DECISIONS.md and LEARNINGS.md might become more complex. Roles might need to be clearly defined (e.g., a documentation owner). These are areas for future exploration.

### AI Assistant’s Perspective

Throughout the project, GitHub Copilot’s “experience” under this framework was indirectly observed through its outputs and behavior. By enforcing clear boundaries, the AI’s contributions became more focused. In effect, the AI behaved like it “understood” the project’s goals better. We quote here two notable reflections that were captured from Copilot’s interactions (as logged via Copilot Chat during development):

> **AI (Copilot) reflection:** “By maintaining clear boundaries in DECISIONS.md, I can concentrate on proposing implementations that align precisely with the human developer’s strategic intent.”

This statement (recorded by the AI in a chat session when asked to summarize its understanding of the project rules) shows that the AI could articulate the value of DECISIONS.md in guiding its actions. It avoided areas explicitly outside its mandate, which likely improved the relevance of its suggestions.

> **AI (Copilot) reflection:** “Consulting the LEARNINGS.md knowledge base allowed me to build upon prior insights, reducing redundant explorations and accelerating solution delivery.”

In another prompted reflection, Copilot acknowledged that LEARNINGS.md entries helped it avoid repeating mistakes. In practice, we did see fewer “naïve” suggestions over time, as the AI seemed to factor in historical fixes (probably because the prompt we provided to Copilot Chat often included content from LEARNINGS.md). These insights illustrate that the framework not only aided the human developers but also enhanced the AI’s performance by providing it with context and memory.

### Comparative Analysis with Other Approaches

It is instructive to compare our human-led, documentation-centric approach to other emerging strategies in AI-assisted development. One alternative approach is to rely on extensive testing and verification to catch AI mistakes, rather than guiding the AI upfront. In such an approach, the AI might generate freely and humans would write tests or use analysis tools to validate outputs. While testing is essential (and we did write extensive unit tests in WallpaperScraper as well), our experience suggests that preventing errors through architectural guidance is more efficient than detecting and fixing them after the fact. By encoding architecture and lessons, we prevented many errors altogether, which is preferable to even the fastest test-driven fix cycle.

Another comparison is with purely human-centric development versus AI-centric with no governance. Compared to a traditional human-only team, our approach achieved far greater raw coding throughput (as evidenced by lines of code produced). Compared to a naive AI-heavy approach (where an AI generates code without special governance and humans try to manage the fallout), our governed model resulted in cleaner architecture and less technical debt. In unmanaged Copilot usage, one might get quick results initially, but as problems accumulate (inconsistencies, lack of rationale, etc.), the cost to fix or refactor could negate the time saved. Our approach aimed to get the best of both worlds: speed of AI with the prudence of human planning. The case study indicates this combination is not only possible but highly effective.

## Future Work

The promising results of this human-led AI development framework open several avenues for future research and refinement:

* **Team Scaling and Collaboration:** Thus far, we demonstrated the framework with a single AI assistant and a lead developer. An important next step is to apply it in a multi-developer team setting, possibly with multiple AI agents (for example, each developer using their own Copilot instance). How does DECISIONS.md evolve when many voices contribute? One could study mechanisms for conflict resolution in ADRs suggested by different team members or even by an AI. Additionally, integrating this approach with agile processes (e.g., during sprint planning and retrospectives, ensuring DECISIONS.md and LEARNINGS.md are updated) would be valuable.

* **Tooling Enhancements:** While our IDE integration was effective, more sophisticated tool support could enhance the experience. For instance, developing a VS Code extension that automatically surfaces relevant DECISIONS.md entries when certain keywords are typed in code, or that warns the developer (and AI) when a suggestion might contradict a decision. Likewise, an intelligent search or summarization assistant for LEARNINGS.md could help developers quickly recall past solutions. This could even be an AI-powered feature: an AI that monitors LEARNINGS.md and proactively reminds the team of similar past issues when a new issue arises.

* **Framework Generalization:** The two-document approach may generalize beyond code generation to other domains of human–AI collaboration. Future work might explore its applicability in contexts like **AI-assisted design** (with a “DesignDecisions.md” and “DesignLearnings.md”) or in data science projects (where decisions about data cleaning and modeling and their outcomes could be recorded). Studying such adaptations could reinforce the general principle of human-guided AI via structured documentation.

* **AI Self-improvement:** An intriguing direction is enabling the AI to not only log learnings but also adapt its own model or prompt based on them. In our case, Copilot’s model was fixed, but we fed it context. Future AI assistants might fine-tune themselves on the content of LEARNINGS.md periodically, effectively *learning* the project’s specifics more deeply. This would blur the line between static documentation and an evolving AI model. Research into safe and effective ways for an AI to incorporate project-specific knowledge (while respecting boundaries) could amplify the benefits we observed.

* **Quantitative Metrics:** Further studies should collect more quantitative data on the impact of the framework. Metrics like the number of architecture violations caught, time saved in code reviews, reduction in bugs introduced, and newcomer onboarding time could all provide concrete evidence of the framework’s value. In our case study, we have mostly qualitative and anecdotal evidence of improvements. A controlled experiment across multiple project teams (some using the two-document framework, some not) would be ideal to measure differences in productivity and code quality outcomes rigorously.

In summary, this work lays a foundation, but much remains to explore in integrating AI into software engineering in a governed, human-centric way. We anticipate that as AI coding assistants become more prevalent, such frameworks will be crucial to ensure that we harness their capabilities responsibly and effectively.

## Conclusion

We presented a human-led, AI-assisted development methodology centered around two living documents: **DECISIONS.md** and **LEARNINGS.md**. This approach creates a structured yet flexible framework for integrating an AI coding assistant into a software project. By building on established practices of documenting architecture decisions (Tyree & Akerman, 2005; Nygard, 2011) and continuous knowledge sharing (IEEE Software, 2002; Martraire, 2019), our framework ensures that AI contributions are made within the bounds of human-approved design and that the rationale for those contributions is continuously documented.

This methodology aligns with recent findings in studies of AI pair programming. It positions the AI as a powerful implementation partner—one that can dramatically accelerate coding—while ensuring that human expertise, creativity, and judgment remain at the center of critical decisions. As highlighted in prior literature (Denny et al., 2023; Jarrahi, 2018), clear role definition and oversight are crucial for effective human-AI collaboration. Our two-document system provides a concrete instantiation of these principles, demonstrating how to keep the *human-in-the-loop* without bottlenecking the AI’s efficiency.

The WallpaperScraper case study serves as a proof of concept of the framework’s viability. The project achieved significant improvements in development speed and resilience, with the vast majority of code produced by the AI under minimal human intervention beyond guidance and review. The fact that nearly the entire codebase was AI-generated yet met quality and architectural standards is a strong testament to the potential of governed AI development. It suggests that, when properly directed, AI assistants can substantially shoulder the burden of coding, freeing human developers to perform higher-level integrative work.

In conclusion, AI-assisted development need not be a choice between **automation and control**. With approaches like the one we propose, it can be a symbiosis: the AI brings speed, and the human provides direction. The two-document approach ensures that even as an AI writes the code, humans *lead* the development process—setting the rules, imparting the lessons of experience, and maintaining the coherence and sustainability of the software. We envision that such frameworks will become increasingly important as AI becomes a ubiquitous part of software engineering. By marrying the strengths of human judgment and AI generation, software teams can achieve new levels of productivity and reliability, heralding a future where AI-assisted development is not only faster but also smarter and more accountable.

## References

Bakal, G., Dasdan, A., Katz, Y., Kaufman, M., & Levin, G. (2025). *Experience with GitHub Copilot for Developer Productivity at ZoomInfo*. arXiv preprint arXiv:2501.13282.

Beck, K., Beedle, M., van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M., … & Thomas, D. (2001). *Manifesto for Agile Software Development*. Agile Alliance. [https://agilemanifesto.org/](https://agilemanifesto.org/)

Bird, C., Zimmermann, T., & Nagappan, N. (2022). The art and science of analyzing software data: AI-assisted software engineering. *Communications of the ACM, 65*(2), 62–71. [https://doi.org/10.1145/3498397](https://doi.org/10.1145/3498397)

Chatterjee, S., Liu, C. L., Rowland, G., & Hogarth, T. (2024). *The impact of an AI tool on engineering at ANZ Bank: An empirical study on GitHub Copilot within a corporate environment*. arXiv preprint arXiv:2401.12345 (In review).

Denny, P., Becker, B. A., Craig, M., Wilson, G., & Ziegler, D. (2023). The impact of AI coding assistants on student learning outcomes. *ACM Queue, 21*(1), 48–64. [https://queue.acm.org/detail.cfm?id=3595878](https://queue.acm.org/detail.cfm?id=3595878)

Hacker News. (2022, July 6). *Discussion on AI code assistants* \[Online forum post]. [https://news.ycombinator.com/item?id=31996413](https://news.ycombinator.com/item?id=31996413)

Henderson, J. P. (2023). *Architecture decision record templates and examples* \[GitHub repository]. [https://github.com/joelparkerhenderson/architecture-decision-record](https://github.com/joelparkerhenderson/architecture-decision-record)

IEEE Software. (2002). **Special Issue on Knowledge Management in Software Engineering**, *19*(3), 26–38.

Jarrahi, M. H. (2018). Artificial intelligence and the future of work: Human–AI symbiosis in organizational decision making. *Business Horizons, 61*(4), 577–586. [https://doi.org/10.1016/j.bushor.2018.03.007](https://doi.org/10.1016/j.bushor.2018.03.007)

Martraire, C. (2019). Living documentation: Continuous knowledge sharing by design. *InfoQ*. [https://www.infoq.com/articles/living-documentation-cyrille-martraire/](https://www.infoq.com/articles/living-documentation-cyrille-martraire/)

Microsoft Research. (2023). *AI pair programming: A new era of software development* \[Project page]. [https://www.microsoft.com/en-us/research/project/ai-pair-programmer/](https://www.microsoft.com/en-us/research/project/ai-pair-programmer/)

Milton, N., & Lambe, P. (2019). *The Knowledge Manager’s Handbook* (2nd ed.). Kogan Page.

Moradi Dakhel, A., Majdinasab, V., Nikanjam, A., Khomh, F., Desmarais, M. C., & Jiang, Z. M. (2023). GitHub Copilot AI pair programmer: Asset or liability? *Journal of Systems and Software, 203*, 111734. [https://doi.org/10.1016/j.jss.2023.111734](https://doi.org/10.1016/j.jss.2023.111734)

Nygard, M. (2011, November 15). Documenting architecture decisions. *Cognitect Blog*. [https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions](https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions)

Prather, J., Reeves, B. N., Denny, P., Becker, B. A., Leinonen, J., Luxton-Reilly, A., Powell, G., Finnie-Ansley, J., & Santos, E. A. (2023). “It’s weird that it knows what I want”: Usability and interactions with Copilot for novice programmers. *ACM Transactions on Computer-Human Interaction, 31*(1), Article 31. [https://doi.org/10.1145/3579483](https://doi.org/10.1145/3579483)

Reddit. (2023a, February 17). *What are your biggest gripes with GitHub Copilot?* \[Online forum post]. [https://www.reddit.com/r/programming/comments/10w7k8g/what\_are\_your\_biggest\_gripes\_with\_github\_copilot/](https://www.reddit.com/r/programming/comments/10w7k8g/what_are_your_biggest_gripes_with_github_copilot/)

Reddit. (2023b, April 12). *What are your biggest gripes with GitHub Copilot?* \[Online forum post]. [https://www.reddit.com/r/ExperiencedDevs/comments/12w7k8g/what\_are\_your\_biggest\_gripes\_with\_github\_copilot/](https://www.reddit.com/r/ExperiencedDevs/comments/12w7k8g/what_are_your_biggest_gripes_with_github_copilot/)

Tools for Architecture Decision Records. (2023). *ADR Tools documentation*. [https://adr.github.io/](https://adr.github.io/)

Tyree, J., & Akerman, A. (2005). Architecture decisions: Demystifying architecture. *IEEE Software, 22*(2), 19–27. [https://doi.org/10.1109/MS.2005.27](https://doi.org/10.1109/MS.2005.27)

Vaithilingam, P., Jain, A., & Saini, R. (2022). Expectations vs. experience: Evaluating the usability of code generation tools powered by large language models. arXiv preprint arXiv:2210.03629.

WallpaperScraper Project. (2025). *GitHub repository*. [https://github.com/fabioc-aloha/WallpaperScraper](https://github.com/fabioc-aloha/WallpaperScraper)

Wang, R., Cheng, R., Ford, D., & Zimmermann, T. (2024). Investigating and designing for trust in AI-powered code generation tools. In *Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT)* (Forthcoming). arXiv preprint arXiv:2305.11248.

Ziegler, A., Kalliamvakou, E., Li, X. A., Rice, A., Rifkin, D., Simister, S., Sittampalam, G., & Aftandilian, E. (2024). Measuring GitHub Copilot’s impact on productivity. *Communications of the ACM, 67*(3), 54–63. [https://doi.org/10.1145/3571884](https://doi.org/10.1145/3571884)

GitHub Copilot. (2025). *AI Assistant’s perspective on the Two-Document Framework.* In WallpaperScraper Project Documentation.
