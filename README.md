![WallpaperScraper Banner](banner.png)

### Abstract

This paper introduces a structured two-document framework designed to foster effective collaboration between human developers and AI coding assistants. By clearly separating strategic decision-making (documented in **DECISIONS.md**) from tactical learning artifacts (documented in **LEARNINGS.md**), development teams can harness AI-generated code suggestions without sacrificing architectural coherence or institutional knowledge. The framework is validated in the context of the **WallpaperScraper** project, where DECISIONS.md captured high-level design choices and LEARNINGS.md recorded iterative improvements and pitfalls. Notably, Copilot spontaneously created several debugging and site investigation scripts to troubleshoot issues encountered during service setup, highlighting the AI's operational role beyond code generation. The paper also details formal recommendations for configuring GitHub Copilot at both the repository and user/workspace levels, including sample implementation files and a comparison of the two approaches. Findings underscore the efficacy of governed humanâ€“AI collaboration, showing reduced architecture-related code review feedback, minimized downtime following external site changes, bolstered developer confidence, and extensive AI-generated contributions under minimal human supervision. **99.8%** of the WallpaperScraper codebase was generated by the AI assistant, illustrating the frameworkâ€™s capacity to leverage AI productivity while maintaining essential human oversight and creativity.

Read the full paper here: [`PAPER.md`](PAPER.md)

# Ultra-Resolution Wallpaper Scraper - AI Assisted Development Case Study

---

## ðŸ“¦ Project Structure Update (June 2025)

**Weâ€™ve reorganized the codebase to make it easier for you to use, extend, and contribute!**

- **All core logic, configuration, and services now live in the `src/` folder.**
- **The root directory only contains the CLI entry point (`main.py`), documentation, and essential project files.**
- **Service modules for each wallpaper site are now under `src/services/`.**
- **Configuration is centralized in `src/config.py`.**
- **Debugging and investigation scripts are in `src/` for easy access.**
- **Dead code removed**: Cleaned up deprecated investigation scripts and duplicate files in v1.0.1.

### Current Layout

```
WallpaperScraper/
â”‚   main.py                # CLI entry point (run this to start)
â”‚   README.md              # Project documentation
â”‚   CHANGELOG.md           # Version history and changes
â”‚   requirements.txt       # Dependencies with version constraints
â”‚   ...                    # Other docs and licenses
â””â”€â”€â”€src/
    â”‚   config.py          # Central configuration
    â”‚   wallpaper_scraper.py, wallpaper_scout.py
    â”‚   investigate_wallpaperswide.py
    â””â”€â”€â”€services/
        â”‚   wallhaven_service.py, wallpaperbat_service.py, wallpaperswide_service.py
```

**To run the scraper, just use:**
```powershell
python main.py --scrape
```

---

## Project Overview

This project serves dual purposes:

1. **Practical Utility**: A Python tool that downloads ultra-high-resolution wallpapers from various sources, making the author's work environment more enjoyable through automated wallpaper curation.

2. **Research Platform**: An experimental testbed for researching effective integration patterns between human developers and GitHub Copilot. The project implements and validates a novel approach to AI-assisted development using `DECISIONS.md` and `LEARNINGS.md` as coordination mechanisms.

## Key Features

- Fetches wallpapers by theme from multiple reputable sources:
  - wallpaperswide.com
  - wallhaven.cc
  - wallpaperbat.com (with specialized support for 5120x1440 super ultrawide wallpapers)
- Supports configurable resolution (default: 5120Ã—1440)
- Sophisticated image resolution verification:
  - Exact match detection (ideal case)
  - Similar aspect ratio with higher resolution
  - Larger resolution with any aspect ratio
  - Automatic removal of undersized images
- Smart file skipping based on both filename and resolution requirements
- Parallel downloads with intelligent retry logic and exponential backoff
- Robust error handling and detailed logging
- Clean, centralized configuration in `config.py`
- Adaptive scraping techniques that handle website structure changes
- Designed to run on Windows PowerShell

## Requirements

Install dependencies in a virtual environment:
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install --upgrade pip
pip install -r requirements.txt
```

**Note**: As of v1.0.1, `requirements.txt` includes proper version constraints and is organized by dependency purpose for better maintainability.

## Performance and Parallelism

The scraper is optimized for speed and efficiency:
- **Parallel Scraping:** Each wallpaper site is scraped in its own thread, so all enabled sites are processed in parallel. This greatly reduces the time to collect wallpaper URLs.
- **Parallel Downloading:** Wallpaper downloads are also performed in parallel, using the same `MAX_WORKERS` setting.
- **Configurable Workers:** The number of parallel threads for both scraping and downloading is controlled by `MAX_WORKERS` in `src/config.py`.

**Example:**
If you set `MAX_WORKERS = 4`, up to 4 sites will be scraped and up to 4 wallpapers will be downloaded at the same time.

## Configuration

All settings are in `src/config.py`:
- `RESOLUTION`: Desired wallpaper size
- `SITES`: Wallpaper sites to fetch from
- `OUTPUT_FOLDER`, `TEMP_FOLDER` paths
- `MAX_WORKERS`: Number of parallel threads for both scraping and downloading
- HTTP settings and parallelism controls

## Usage

Run the scraper in PowerShell:
```powershell
# Single theme
python main.py --scrape --theme nature

# Multiple themes (space-separated)
python main.py --scrape --theme nature abstract city

# Multi-word themes (use quotes for themes with spaces)
python main.py --scrape --theme "new york" landscape "mountain lake"
```

When run without arguments, you'll be prompted to enter themes:
```
Enter themes for wallpaper search (separated by spaces, use quotes for multi-word themes): nature abstract "mountain lake"
```

Downloaded wallpapers are saved under the configured `OUTPUT_FOLDER`.

## Notes
- **Themes are provided via CLI or interactive prompt** (not in config.py)
- Multiple themes must be space-separated (do not use commas)
- Use quotes for themes that contain spaces
- Each theme will be searched independently across all configured sites
- All output uses logging (no print statements).
- The CLI supports `--theme`, `--resolution`, and `--log-level` options.

## Resolution Verification

The scraper uses a sophisticated resolution matching system with four quality levels:

1. **Exact Match (Level 3)**
   - Image dimensions match the target resolution within 5% tolerance
   - Ideal for maintaining consistent wallpaper collection

2. **Similar Aspect Ratio (Level 2)**
   - Larger resolution than target
   - Aspect ratio within 10% of target
   - Good for high-quality alternatives

3. **Larger Resolution (Level 1)**
   - Exceeds target dimensions
   - Different aspect ratio
   - Acceptable when exact matches unavailable

4. **Insufficient (Level 0)**
   - Smaller than target resolution
   - Automatically removed to maintain quality

This system ensures you get the best possible wallpapers while maintaining flexibility when exact matches aren't available.

## Running Tests

To run the test suite:
```powershell
python -m pytest tests/
```

The test suite covers:
- Download functionality with mock HTTP responses
- Resolution verification with various image sizes
- Error handling and retry logic
- Service-specific functionality for each wallpaper site

For development, you can also run individual test files:
```powershell
python -m pytest tests/test_wallpaper_scraper.py
python -m pytest tests/test_wallpaperbat_service.py
```

## Architecture

Decisions and architectural rationale are documented in `DECISIONS.md`.

## Project Evolution

This project has evolved through careful consideration of various challenges and solutions. We maintain two key documents that not only track our journey but also help GitHub Copilot provide better assistance:

1. `DECISIONS.md` - Contains architectural decisions that require explicit user approval to change. This helps GitHub Copilot understand and respect the project's core design principles.

2. `LEARNINGS.md` - Automatically updated by GitHub Copilot as it helps solve problems, documenting successful approaches and preventing repetition of unsuccessful strategies.

### Design Decisions (`DECISIONS.md`)

Key architectural decisions include:
- Multi-service architecture for resilience against site changes
- Sophisticated resolution matching system with quality levels
- Parallel processing with intelligent retry mechanisms
- Centralized configuration for easy customization

See [`DECISIONS.md`](DECISIONS.md) for detailed rationales behind these and other architectural choices.

### Project Learnings (`LEARNINGS.md`)

Notable learnings from development include:
- Website scraping challenges and solutions
- Service-specific adaptations
- Performance optimization techniques
- Error handling strategies
- Testing approaches

Our [`LEARNINGS.md`](LEARNINGS.md) document captures these experiences in detail, helping future contributors understand:
- What worked well
- What didn't work
- How we solved specific challenges
- Ongoing improvements

### Integration with GitHub Copilot

These documents serve a dual purpose:

1. **Human Documentation**: They provide valuable context for users and contributors about the project's evolution and rationale.

2. **AI Assistance**: They help GitHub Copilot:
   - Understand and respect architectural boundaries
   - Learn from past solutions and challenges
   - Make informed suggestions aligned with project principles
   - Avoid repeating approaches that didn't work
   - Document new solutions as they're implemented

`DECISIONS.md` acts as a guardrail, requiring explicit user approval for architectural changes, while `LEARNINGS.md` serves as GitHub Copilot's dynamic knowledge base, continuously updated as it assists with problem-solving.

## Debugging and Site Structure Investigation Scripts

The project includes a dedicated script for investigating and debugging the structure of wallpaper sites. **Note**: As of v1.0.1, deprecated scripts targeting the non-functional 4kwallpapers.com site have been removed to keep the codebase clean and focused.

### Why These Scripts Exist
- **Websites change frequently:** Scraping logic can break if a site updates its HTML structure, class names, or navigation patterns.
- **Rapid prototyping:** Before implementing a new service or fixing a broken one, it's essential to quickly understand the current structure of the target site.
- **Selector discovery:** These scripts help identify the best CSS selectors and patterns for reliably extracting wallpaper links and download URLs.
- **Documentation and reproducibility:** By saving HTML and printing out findings, these scripts document the investigation process, making it easier for future contributors to understand how scraping logic was developed.

### Available Investigation Script

- **`investigate_wallpaperswide.py`:**
  - Tries multiple approaches (by resolution, by category, homepage) to fetch and analyze wallpaperswide.com.
  - Saves HTML responses for offline inspection.
  - Searches for common elements and wallpaper links using both direct and indirect methods.
  - Visits detail pages, tries a variety of selectors to find download links, and prints out resolution information.
  - Documents which selectors and patterns are most effective for the current site structure.

### When to Use
- When adding support for a new wallpaper site.
- When an existing service breaks due to site changes.
- When optimizing or refactoring scraping logic.
- When investigating new potential wallpaper sources.

### Creating New Investigation Scripts
When adding support for new sites, follow the pattern established by `investigate_wallpaperswide.py`:
1. Fetch target pages and save HTML for offline analysis
2. Try multiple selector strategies
3. Document findings and effective patterns
4. Test resolution extraction and link discovery

These scripts are essential for maintaining the scraper's resilience and for onboarding new contributors who need to understand the rationale behind selector choices and scraping strategies.

## Version History

For detailed information about changes, improvements, and bug fixes in each version, see [`CHANGELOG.md`](CHANGELOG.md).

**Current Version**: 1.1.0 - Major feature enhancements including environment variables, advanced CLI, error handling, and comprehensive testing.

## License

The code in this repository is licensed under the MIT License (see `LICENSE`).

The research paper (`PAPER.md`) is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (see `LICENSE-PAPER.md`).