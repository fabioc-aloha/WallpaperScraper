# Ultra-Resolution Wallpaper Scraper - AI Assisted Development Case Study

## Research Paper: AI-Assisted Development: A Human-Led Approach

This project serves as both a practical tool and a research platform, leading to the publication of a paper on human-AI collaborative development. The paper presents a novel framework for AI-assisted software development using two key documents (`DECISIONS.md` and `LEARNINGS.md`) to create a structured relationship between human developers and AI assistants.

### Abstract

This paper introduces a structured two-document framework designed to foster effective collaboration between human developers and AI coding assistants. By clearly separating strategic decision-making (documented in **DECISIONS.md**) from tactical learning artifacts (documented in **LEARNINGS.md**), development teams can harness AI-generated code suggestions without sacrificing architectural coherence or institutional knowledge. The framework is validated in the context of the **WallpaperScraper** project, where DECISIONS.md captured high-level design choices and LEARNINGS.md recorded iterative improvements and pitfalls. Notably, Copilot spontaneously created several debugging and site investigation scripts to troubleshoot issues encountered during service setup, highlighting the AI's operational role beyond code generation. The paper also details formal recommendations for configuring GitHub Copilot at both the repository and user/workspace levels, including sample implementation files and a comparison of the two approaches. Findings underscore the efficacy of governed human–AI collaboration, showing reduced architecture-related code review feedback, minimized downtime following external site changes, bolstered developer confidence, and extensive AI-generated contributions under minimal human supervision. **99.8%** of the WallpaperScraper codebase was generated by the AI assistant, illustrating the framework’s capacity to leverage AI productivity while maintaining essential human oversight and creativity.

Read the full paper here: [`PAPER.md`](PAPER.md)

## Project Overview

This project serves dual purposes:

1. **Practical Utility**: A Python tool that downloads ultra-high-resolution wallpapers from various sources, making the author's work environment more enjoyable through automated wallpaper curation.

2. **Research Platform**: An experimental testbed for researching effective integration patterns between human developers and GitHub Copilot. The project implements and validates a novel approach to AI-assisted development using `DECISIONS.md` and `LEARNINGS.md` as coordination mechanisms.

## Key Features

- Fetches wallpapers by theme from multiple reputable sources:
  - wallpaperswide.com
  - wallhaven.cc
  - wallpaperbat.com (with specialized support for 5120x1440 super ultrawide wallpapers)
- Supports configurable resolution (default: 5120×1440)
- Sophisticated image resolution verification:
  - Exact match detection (ideal case)
  - Similar aspect ratio with higher resolution
  - Larger resolution with any aspect ratio
  - Automatic removal of undersized images
- Smart file skipping based on both filename and resolution requirements
- Parallel downloads with intelligent retry logic and exponential backoff
- Robust error handling and detailed logging
- Clean, centralized configuration in `config.py`
- Adaptive scraping techniques that handle website structure changes
- Designed to run on Windows PowerShell

## Requirements

Install dependencies in a virtual environment:
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install --upgrade pip
pip install -r requirements.txt
```

## Configuration

All settings are in `config.py`:
- `RESOLUTION`: Desired wallpaper size
- `THEMES`: List of search themes
- `SITES`: Wallpaper sites to fetch from
- `OUTPUT_FOLDER`, `TEMP_FOLDER` paths
- HTTP settings and parallelism controls

## Usage

Run the scraper in PowerShell:
```powershell
python wallpaper_scraper.py
```

Downloaded wallpapers are saved under the configured `OUTPUT_FOLDER`.

## Resolution Verification

The scraper uses a sophisticated resolution matching system with four quality levels:

1. **Exact Match (Level 3)**
   - Image dimensions match the target resolution within 5% tolerance
   - Ideal for maintaining consistent wallpaper collection

2. **Similar Aspect Ratio (Level 2)**
   - Larger resolution than target
   - Aspect ratio within 10% of target
   - Good for high-quality alternatives

3. **Larger Resolution (Level 1)**
   - Exceeds target dimensions
   - Different aspect ratio
   - Acceptable when exact matches unavailable

4. **Insufficient (Level 0)**
   - Smaller than target resolution
   - Automatically removed to maintain quality

This system ensures you get the best possible wallpapers while maintaining flexibility when exact matches aren't available.

## Running Tests

To run the test suite:
```powershell
python -m pytest tests/
```

The test suite covers:
- Download functionality with mock HTTP responses
- Resolution verification with various image sizes
- Error handling and retry logic
- Service-specific functionality for each wallpaper site

For development, you can also run individual test files:
```powershell
python -m pytest tests/test_wallpaper_scraper.py
python -m pytest tests/test_wallpaperbat_service.py
```

## Architecture

Decisions and architectural rationale are documented in `DECISIONS.md`.

## Project Evolution

This project has evolved through careful consideration of various challenges and solutions. We maintain two key documents that not only track our journey but also help GitHub Copilot provide better assistance:

1. `DECISIONS.md` - Contains architectural decisions that require explicit user approval to change. This helps GitHub Copilot understand and respect the project's core design principles.

2. `LEARNINGS.md` - Automatically updated by GitHub Copilot as it helps solve problems, documenting successful approaches and preventing repetition of unsuccessful strategies.

### Design Decisions (`DECISIONS.md`)

Key architectural decisions include:
- Multi-service architecture for resilience against site changes
- Sophisticated resolution matching system with quality levels
- Parallel processing with intelligent retry mechanisms
- Centralized configuration for easy customization

See [`DECISIONS.md`](DECISIONS.md) for detailed rationales behind these and other architectural choices.

### Project Learnings (`LEARNINGS.md`)

Notable learnings from development include:
- Website scraping challenges and solutions
- Service-specific adaptations
- Performance optimization techniques
- Error handling strategies
- Testing approaches

Our [`LEARNINGS.md`](LEARNINGS.md) document captures these experiences in detail, helping future contributors understand:
- What worked well
- What didn't work
- How we solved specific challenges
- Ongoing improvements

### Integration with GitHub Copilot

These documents serve a dual purpose:

1. **Human Documentation**: They provide valuable context for users and contributors about the project's evolution and rationale.

2. **AI Assistance**: They help GitHub Copilot:
   - Understand and respect architectural boundaries
   - Learn from past solutions and challenges
   - Make informed suggestions aligned with project principles
   - Avoid repeating approaches that didn't work
   - Document new solutions as they're implemented

`DECISIONS.md` acts as a guardrail, requiring explicit user approval for architectural changes, while `LEARNINGS.md` serves as GitHub Copilot's dynamic knowledge base, continuously updated as it assists with problem-solving.

## Debugging and Site Structure Investigation Scripts

Note: These scripts were created "spontaneously" by Copilot as tools to ensure robust scraping and adaptability to site changes, this project includes dedicated scripts for investigating and debugging the structure of wallpaper sites:

### Why These Scripts Exist
- **Websites change frequently:** Scraping logic can break if a site updates its HTML structure, class names, or navigation patterns.
- **Rapid prototyping:** Before implementing a new service or fixing a broken one, it's essential to quickly understand the current structure of the target site.
- **Selector discovery:** These scripts help identify the best CSS selectors and patterns for reliably extracting wallpaper links and download URLs.
- **Documentation and reproducibility:** By saving HTML and printing out findings, these scripts document the investigation process, making it easier for future contributors to understand how scraping logic was developed.

### How the Scripts Work
- **`debug_site_structure.py`:**
  - Fetches a target page (e.g., a category or search result page).
  - Prints out the page title, number of images, and tries multiple CSS selectors to find wallpaper thumbnails and detail links.
  - Examines the first few links and visits a detail page to probe for download links using various selectors.
  - Outputs findings to the console for rapid iteration.

- **`investigate_wallpaperswide.py`:**
  - Tries multiple approaches (by resolution, by category, homepage) to fetch and analyze wallpaperswide.com.
  - Saves HTML responses for offline inspection.
  - Searches for common elements and wallpaper links using both direct and indirect methods.
  - Visits detail pages, tries a variety of selectors to find download links, and prints out resolution information.
  - Documents which selectors and patterns are most effective for the current site structure.

- **`investigate_wallpapers.py`:**
  - Focuses on 4kwallpapers.com, especially for a given theme (e.g., 'nature').
  - Fetches a theme/category page, saves the HTML for offline inspection, and analyzes image thumbnails and their parent links to discover detail pages.
  - Identifies potential wallpaper detail pages by URL patterns and visits them to extract download links, resolution information, and direct image sources.
  - Prints and saves findings, helping to reverse-engineer the site's navigation and download structure for robust scraper implementation.

### When to Use
- When adding support for a new wallpaper site.
- When an existing service breaks due to site changes.
- When optimizing or refactoring scraping logic.

These scripts are essential for maintaining the scraper's resilience and for onboarding new contributors who need to understand the rationale behind selector choices and scraping strategies.

## License

The code in this repository is licensed under the MIT License (see `LICENSE`).

The research paper (`PAPER.md`) is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (see `LICENSE-PAPER.md`).